{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üó£Ô∏è ***A Greater Understanding Brought To You By This Article: https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use text in ML models:\n",
    "\n",
    "Text data requires special preparation before you can start using it for predictive modeling.\n",
    "\n",
    "The text must be parsed to remove words, called tokenization. Then the words need to be encoded as integers or floating point values for use as input to a machine learning algorithm, called feature extraction (or vectorization).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes cover:\n",
    "- How to convert text to word count vectors with CountVectorizer.\n",
    "- How to convert text to word frequency vectors with TfidfVectorizer.\n",
    "- How to convert text to unique integers with HashingVectorizer.\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words Model\n",
    "https://machinelearningmastery.com/gentle-introduction-bag-words-model/\n",
    "\n",
    "The model is simple in that it throws away all of the order information in the words and focuses on the occurrence of words in a document.\n",
    "\n",
    "we are only concerned with encoding schemes that represent what words are present or the degree to which they are present in encoded documents without any information about order."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Word Counts with CountVectorizer üçé"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "The **CountVectorizer** provides a simple way to \n",
    "1. tokenize a collection of text documents \n",
    "2. build a vocabulary of known words \n",
    "3. encode new documents using that vocabulary\n",
    "\n",
    "***Steps:***\n",
    "1. Create an instance of the ```CountVectorizer``` class.\n",
    "2. Call the ```fit()``` function in order to learn a vocabulary from one or more documents.\n",
    "3. Call the ```transform()``` function on one or more documents as needed to encode each as a vector.\n",
    "\n",
    "An **encoded vector** is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document.\n",
    "\n",
    "Because these vectors will contain a lot of zeros, we call them **sparse**. Python provides an efficient way of handling sparse vectors in the ```scipy.sparse package```.\n",
    "\n",
    "The vectors returned from a call to ```transform()``` will be *sparse vectors*, and you can transform them back to numpy arrays to look and better understand what is going on by calling the ```toarray()``` function.\n",
    "\n",
    "Below is an example of using the ```CountVectorizer``` to tokenize, build a vocabulary, and then encode a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "...learning from the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# access the vocabulary to see what exactly was tokenize\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can see that all words were made lowercase by default and that the punctuation was ignored. These and other aspects of tokenizing can be configured and I encourage you to review all of the options in the API documentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    " ... encode text into a sparse vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector shape:  (1, 8)\n"
     ]
    }
   ],
   "source": [
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "print(\"vector shape: \",vector.shape) # 8 words in the text and 1 body of text therefore encoded vectors have a length of 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "# summarize encoded vector\n",
    "# print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then see that the encoded vector is a **sparse matrix**. Finally, we can see an array version of the encoded vector showing a count of 1 occurrence for each word except the (index and id 7) that has an occurrence of 2.\n",
    "\n",
    "\n",
    "***Importantly***, the same vectorizer can be used on documents that contain words not included in the vocabulary. These words are ignored and no count is given in the resulting vector.\n",
    "\n",
    "For example, below is an example of using the vectorizer above to encode a document with one word in the vocab and one word that is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode another document\n",
    "text2 = [\"the puppy\"]\n",
    "vector = vectorizer.transform(text2)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this example prints the array version of the encoded sparse vector showing one occurrence of the one word in the vocab and the other word not in the vocab completely ignored.\n",
    "[[0 0 0 0 0 0 0 1]]\n",
    "The encoded vectors can then be used directly with a machine learning algorithm.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü••ü§Ø \n",
    "SoOOooo what that means it's best to use a vectorizor that that has encoded or \"learned\" as many words as possible so no matter what body of text we transform, the sparse matrix returned won't be all zeros and the unlearned words in thr text won't be ignored ü§ì\n",
    "\n",
    "the last element in the matrix cooresponds to the word \"the\" that was originally learned at the ```vectorizer.transform(text)``` stage. In ```text2```, there is only 1 occurance of the word \"the\" üòÅ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Word Frequencies with TfidfVectorizer üçä"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word counts are a good starting point, but are very basic.\n",
    "\n",
    "One issue with simple counts is that some words like ‚Äúthe‚Äù will appear many times and their large counts will not be very meaningful in the encoded vectors.\n",
    "\n",
    "An alternative is to **calculate word frequencies**, and by far the most popular method is called ***TF-IDF***. This is an acronym than stands for *‚ÄúTerm Frequency ‚Äì Inverse Document‚Äù Frequency* which are the components of the resulting scores assigned to each word.\n",
    "\n",
    "* **Term Frequency:** This summarizes how often a given word appears within a document.\n",
    "* **Inverse Document Frequency:** This downscales words that appear a lot across documents.\n",
    "\n",
    "\n",
    "Without going into the math, ***TF-IDF are word frequency scores that try to highlight words that are more interesting***, e.g. frequent in a document but not across documents.\n",
    "\n",
    "The TfidfVectorizer will \n",
    "- tokenize documents\n",
    "- learn the vocabulary \n",
    "- inverse document frequency weightings \n",
    "- allow you to encode new documents. \n",
    "\n",
    "Alternately, if you already have a learned CountVectorizer, you can use it with a TfidfTransformer to just calculate the inverse document frequencies and start encoding documents.\n",
    "\n",
    "The same create, fit, and transform process is used as with the CountVectorizer.\n",
    "\n",
    "Below is an example of using the TfidfVectorizer to learn vocabulary and inverse document frequencies across 3 small documents and then encode one of those documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "    \"The dog.\",\n",
    "    \"The fox\"]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the Inverse document frequency vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the array version of our encoded vector for the first document.\n",
    "\n",
    "inverse document frequency weightings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hashing with HashingVectorizer üçí"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts and frequencies can be very useful, but one limitation of these methods is that the vocabulary can become very large.\n",
    "\n",
    "This, in turn, will require large vectors for encoding documents and impose large requirements on memory and slow down algorithms.üëé\n",
    "\n",
    "A clever work around is to **use a one way hash of words to convert them to integers**. The clever part is that no vocabulary is required and you can choose an arbitrary-long fixed length vector. \n",
    "\n",
    "A downside is that **the hash is a one-way function** so there is no way to convert the encoding back to a word (which may not matter for many supervised learning tasks).\n",
    "\n",
    "The ```HashingVectorizer``` class implements this approach that can be used to consistently hash words, then tokenize and encode documents as needed.\n",
    "\n",
    "The example below demonstrates the ```HashingVectorizer``` for encoding a single document.\n",
    "\n",
    "***Note:*** This vectorizer does not require a call to fit on the training data documents. Instead, after instantiation, it can be used directly to start encoding documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An arbitrary fixed-length vector size of 20 was chosen. This corresponds to the range of the hash function, where small values (like 20) may result in hash collisions. *Remembering back to compsci classes, I believe there are heuristics that you can use to pick the hash length and probability of collision based on estimated vocabulary size.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèéÔ∏èüèÅ Finished üèéÔ∏èüèÅ\n",
    "\n",
    "Look what you've learned today üëèü§ì"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
